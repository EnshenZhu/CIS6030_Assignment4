---
title: "R Notebook"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

library(ClusterR)
# unload all residual packages
invisible(lapply(paste0("package:", names(sessionInfo()$otherPkgs)),   # Unload add-on packages
                 detach,
                 character.only = TRUE, unload = TRUE))
```
```{r}
# install.packages("R6")
# install.packages("ClusterR")
# install.packages("cluster")
# install.packages("DescTools")
# install.packages("igraph")
# install.packages("dbscan")
# install.packages("r2r")
# install.packages("datastructures")

# Loading package
library(ClusterR)
library(cluster)
library(DescTools)
library(igraph)
import::from(R6, R6Class)
library(r2r)
```
k_param and MCS need be to declared manually, we declare them at very top
```{r}
k_param <- 3 # the core distance of a point is the distance from the point to the k-th nearest neighbour.
MCS <- 5 # minimum cluster size
```
```{r}
# import the iris data from the docker
result <- system("docker exec namenode hdfs dfs -cat ./input/iris.csv", intern = TRUE)
writeLines(result, "./hdbscan_manual/temp_iris.csv")
iris_data <- read.csv("./hdbscan_manual/temp_iris.csv",
                      header = FALSE,
                      colClasses = c(NA, NA, NA, NA, NA),
                      col.names = c("sepal_length", "sepal_width", "petal_length", "petal_width", "species"))
```
```{r}
# subtract the iris_params, which only contain the sepal and petal numberical values
iris_params <- iris_data[, -5]
iris_params
```

## Part 1 - Work out the core distance, mutual reachability distance and mutual reachability distance graph

```{r}
# create distance matrix for each data point pairs
L2_dist_matrix = as.matrix(dist(iris_params, method = "euclidean"))
L2_dist_matrix
```
```{r}
# caluclate the core ditance for each data point base on parameter k
core_dist <- apply(L2_dist_matrix, 1, FUN = function(x) Small(x, k = k_param + 1, unique = FALSE, na.last = NA)[k_param])

core_dist[1]
```
```{r}

# build the mutal distance matrix
dp_num = dim(iris_params)[1]
mutal_dist_matrix <- matrix(, nrow = dp_num, ncol = dp_num)

for (row_idx in 1:dp_num) {
  for (col_idx in 1:dp_num) {
    if (row_idx != col_idx) {
      #mutal distance is the max of (L2_dist(x,y), core_dist(x),core_dist(y))
      mutal_dist_matrix[row_idx, col_idx] <- max(L2_dist_matrix[row_idx, col_idx], core_dist[row_idx], core_dist[col_idx])
    }
    else {
      mutal_dist_matrix[row_idx, col_idx] = 0
    }
  }
}

# mutal_dist_matrix
```

## Part II Build the minimum spanning tree

```{r}
temp_graph = mutal_dist_matrix
# Step 1: Convert the matrix into the upper triangle matrix firstly
temp_graph[upper.tri(temp_graph)] <- 0

temp_graph_undirected <- as.undirected(graph.adjacency(temp_graph, weighted = TRUE))
graph_MST <- mst(temp_graph_undirected)
# graph_MST
```
```{r}
# Step 2: Turn back to the triangle matrix
temp_adjacent_matrix <- as_adjacency_matrix(graph_MST, type = "lower", attr = "weight")
MinimumSpanning_matrix <- matrix(temp_adjacent_matrix, nrow = dim(temp_adjacent_matrix)[1])
MinimumSpanning_matrix[upper.tri(MinimumSpanning_matrix)] <- 0
MinimumSpanning_matrix # Here is teh minimum spanning tree in the matrix format
```

## Part III
### Step0: Preset the class of the hieratical tree

```{r}
# MutualDistanceNode will be the internal nodes
TreeNodes <- R6Class("TreeNodes",
                     public = list(distance_val = NULL,
                                   leftNodeIdx = NULL,
                                   rightNodeIdx = NULL,

                                   selfIdx = NULL,
                                   parentNodeIdx = NULL,

                                   # The following fields are for the Condense and Extract
                                   hasChildren = list(),
                                   potentialNoise = FALSE,
                                   globalNoise = FALSE,
                                   lamda_birth = NULL,
                                   lamda_death = NULL,
                                   stability = NULL,
                                   clusterBelongIdx = NULL,

                                   initialize = function(distance_val = NULL, left = NULL, right = NULL, selfIdx = NULL, parentNodeIdx = NULL) {

                                     self$distance_val = distance_val
                                     self$leftNodeIdx = left
                                     self$rightNodeIdx = right

                                     self$selfIdx = selfIdx
                                     self$parentNodeIdx = parentNodeIdx
                                   }
                     ))
```

### Step1: Get all edges, their corresponded element pairs, and sort them

```{r}
#RECALL dp_num should be the dimension size -> 150

# Create a distance dataFrame to store the (distance,x_coor,y_coor) info
N <- dp_num - 1 # pre-allocate the dp_num of rows <this might be overestimate> SHOULD BE 150 Lines
dist_DF <- data.frame(core_dist = rep(NA, N), point1_idx = rep(NA, N), point2_idx = rep(NA, N), # we need three columns
                      stringsAsFactors = FALSE) # you don't know levels yet

i <- 1
for (row_idx in 1:dp_num) {
  for (col_idx in 1:dp_num) {
    if (MinimumSpanning_matrix[row_idx, col_idx] != 0) {
      # add the (distance,x_coor,y_coor) to the dataFrame
      row_idx <- as.integer(row_idx)
      col_idx <- as.integer(col_idx)
      dist_DF[i,] <- list(MinimumSpanning_matrix[row_idx, col_idx], row_idx, col_idx)
      i <- i + 1
    }
  }
}

dist_DF <- dist_DF[order(dist_DF$core_dist),]
dist_DF

```
## Step2 Generate a hashmap to store all nodes (including leaf nodes and internal nodes
```{r}
allNode_Map <- hashmap()

for (idx in (-dp_num + 1):dp_num) {
  idx_char = as.character(idx)
  insert(allNode_Map, idx_char, TreeNodes$new(selfIdx = idx, parentNodeIdx = idx))

  if (idx > 0) { # when looping at leaf nodes
    temp_NodeObj <- query(allNode_Map, idx_char)
    temp_NodeObj$hasChildren <- append(temp_NodeObj$hasChildren, idx) # the children list to be itself
    temp_NodeObj$stability <- 0 # leaf node's stability should be 0
  }
}

testObj <- query(allNode_Map, "10")
testObj
```

## Step 3 Define the Union and Find functions

A. The following is the O(n) Find and O(1) Union

```{r}
FindNode <- function(x, allNode_Map) {
  # x -> TreeNode Class
  # allNode_Map -> hashmap

  if (x$parentNodeIdx == x$selfIdx) {
    return(x$selfIdx)
  }

  parentNode <- query(allNode_Map, as.character(x$parentNodeIdx))
  return(FindNode(parentNode, allNode_Map))
}

UnionNode <- function(targetIdx, x, y, allNode_Map) {
  # targetIdx -> int
  # x -> TreeNode Class
  # y -> TreeNode Class

  x$parentNodeIdx <- targetIdx
  y$parentNodeIdx <- targetIdx

  #store all corresponded leaf nodes from two children nodes
  tempObj <- query(allNode_Map, as.character(targetIdx))
  tempObj$hasChildren <- c(tempObj$hasChildren, x$hasChildren, y$hasChildren)
}
```

B. Iterate the dist_DF table to build the tree

```{r}
tracker <- 1 # tracking the dist_DF dataFrame table

for (idx in -1:(-dp_num + 1)) {
  the_dist <- dist_DF[tracker, 1]
  the_pointA_idx <- as.integer(dist_DF[tracker, 2])
  the_pointB_idx <- as.integer(dist_DF[tracker, 3])

  NodeObj <- query(allNode_Map, as.character(idx))

  # print(the_pointA_idx[[1]])
  # print(the_pointB_idx[[1]])

  NodeObj$distance_val <- the_dist

  # config the node object to be unioned
  the_pointA <- query(allNode_Map, as.character(the_pointA_idx))
  the_pointB <- query(allNode_Map, as.character(the_pointB_idx))

  NodeObj$leftNodeIdx <- FindNode(the_pointA, allNode_Map)
  NodeObj$rightNodeIdx <- FindNode(the_pointB, allNode_Map)

  leftNode <- query(allNode_Map, as.character(NodeObj$leftNodeIdx))
  rightNode <- query(allNode_Map, as.character(NodeObj$rightNodeIdx))

  UnionNode(targetIdx = idx, x = leftNode, y = rightNode, allNode_Map = allNode_Map)

  tracker <- tracker + 1

}

```
```{r}
# query(allNode_Map, "-148")
length(query(allNode_Map, "-148")$hasChildren)
# query(allNode_Map, "13")
```
## Step4 Do condense

Before start - when doing the Union Find, store all the corresponded Leaf Nodes and put them into the hasChildren list

A. Do the Level Order Traversal of the internal nodes of Hieratical Tree 进行层序遍历
Recall that the idx range of internal nodes are from -dp_num+1 to -1

B. Mark out the internal nodes which has the num of children less than MCS. They will become potential noise points

C. When doing the traversal, we also need to calculate the corresponded lamda (=1/distance)
```{r}
# datastructures seem to have conflict with the previous query method,
# so we import the library here rather than at top
library(datastructures)
```
```{r}
query(allNode_Map, "2")
```
```{r}
q <- queue() # for doing DFS (Level Order Traversal)

temp_NodeObj <- query(allNode_Map, as.character(-dp_num + 1))
q <- insert(q, temp_NodeObj)

while (size(q) > 0) {
  first_NodeObj <- peek(q) # get the first element of the queue

  if (first_NodeObj$selfIdx < 0) { # we are only going to process internal nodes (idx<0)
    # mark it as the potentialNoise if its nums of children is smaller than MCS
    if (length(first_NodeObj$hasChildren) < MCS) {
      first_NodeObj$potentialNoise <- TRUE
      print(first_NodeObj$selfIdx)
    }

    first_NodeObj$lamda_birth <- (1 / first_NodeObj$distance_val) # record the lamda_birth

    next_leftNodeIdx <- first_NodeObj$leftNodeIdx
    next_leftNode <- query(allNode_Map, as.character(next_leftNodeIdx))
    q <- insert(q, next_leftNode)

    next_rightNodeIdx <- first_NodeObj$rightNodeIdx
    next_rightNode <- query(allNode_Map, as.character(next_rightNodeIdx))
    q <- insert(q, next_rightNode)
  }

  pop(q)
}

```
```{r}
query(allNode_Map, "-22")
```
## Step 5 Do Extract
### A. Create a nodeExtractWork function
* For leaf node, lambda_death = lambda_birth
* For internal node, if it is not potential outlier -> Lambda_death = (num_children )* (left_child_lambda_brith);
* else Lambda_death = left_child_death + right_child_death
```{r}
nodeExtractWork <- function(the_Node, allNode_Map) {

  # visit the left and right node
  the_leftNode <- query(allNode_Map, as.character(root$leftNodeIdx))
  the_rightNode <- query(allNode_Map, as.character(root$rightNodeIdx))

  # section 1 Calculate Node lambda_death for each internal node
  if (the_Node$selfIdx >= 0) { # if the node is a leaf node
    the_Node$lamda_death = the_Node$lamda_birth #
  }
  else { # if the node is an internal node

    # For internal node, if it is not potential outlier -> Lambda_death = (num_children )* (left_child_lambda_brith);
    if (the_leftNode$potentialNoise == FALSE & the_rightNode$potentialNoise == FALSE) {
      the_Node$lamda_death = (length(the_Node$hasChildren) * the_leftNode$lamda_birth)
    }

      # if one of the child node is a potential outlier, Lambda_death = left_child_death + right_child_death
    else {
      the_Node$lamda_death = the_leftNode$lamda_death + the_rightNode$lamda_death
    }
  }

  # section 2 Calculate Node Stability
  the_Node$stability <- the_Node$lamda_death - length(the_Node$hasChildren) * the_Node$lamda_birth

  # # section 3 Extract and find global noise
  if (the_Node$stability >= (the_leftNode$stability + the_rightNode$stability)) {

    # if merge is more stable, we mark the two children into the same cluster idx
    the_leftNode$clusterBelongIdx <- the_Node$selfIdx
    the_rightNode$clusterBelongIdx <- the_Node$selfIdx
  }
  # else{
  #   # if split is more stable, we mark the two children into the same cluster idx
  # }
}
```
```{r}
# # The followed post-order traversal is derived from https://zhuanlan.zhihu.com/p/80578741
#
# posOrderRecur <- function(root, allNode_Map) {
#   theCache <- list()
#
#   s <- stack()
#   cur <- root
#   pre <- NULL # 记录上一次访问的节点
#
#   while (!is.null(cur) | length(s) != 0) {
#     while (!is.null(cur)) {
#       s <- insert(s, cur)
#       the_leftNode <- query(allNode_Map, as.character(cur$leftNodeIdx))
#       cur <- the_leftNode
#     }
#
#     if (length(s) != 0) {
#       cur <- peek(s)
#       pop(s) # recall that pop is an anonimous functrion in datastructure library
#
#       the_rightNode <- query(allNode_Map, as.character(cur$rightNodeIdx))
#
#       if (is.null(the_rightNode) | pre$selfIdx == the_rightNode$selfIdx) { # 访问节点的条件
#         # theCache <- append(theCache, cur) # 访问节点
#         print(cur$selfIdx)
#         pre <- cur # 这一步是记录上一次访问的节点
#         cur <- NULL # 此处为了跳过下一次循环的访问左子节点的过程，直接进入栈的弹出阶段，因为但凡在栈中的节点，它们的左子节点都肯定被经过且已放入栈中。
#       }
#       else { #不访问节点的条件
#         s <- insert(s, cur)
#         cur <- the_rightNode
#       }
#     }
#   }
#
#   # return(theCache)
#
# }
```
```{r}
# node1 <- TreeNodes$new(selfIdx = 1, left = 2, right = 3)
# node2 <- TreeNodes$new(selfIdx = 2, left = NULL, right = NULL)
# node3 <- TreeNodes$new(selfIdx = 3, left = NULL, right = NULL)
#
# tempMap <- hashmap()
# insert(tempMap, "1", node1)
# insert(tempMap, "2", node2)
# insert(tempMap, "3", node3)
#
# posOrderRecur(node1, tempMap)

```
### B. Create A postorder traversal function (we use recursive design in this case)
```{r}
# In recursion

theCache <- list()

posOrderRecur <- function(root, allNode_Map, cache) {

  # root -> TreeNode
  if (!is.null(root)) {


    # visit the left node
    the_leftNode <- query(allNode_Map, as.character(root$leftNodeIdx))
    cache <- posOrderRecur(the_leftNode, allNode_Map, cache)

    # visit the right node
    the_rightNode <- query(allNode_Map, as.character(root$rightNodeIdx))
    cache <- posOrderRecur(the_rightNode, allNode_Map, cache)

    cache <- append(cache, root)
    return(cache)
  }

}

```
```{r}
start_root <- query(allNode_Map, as.character(-dp_num + 1))
theCache <- posOrderRecur(start_root, allNode_Map, theCache)
theCache
```
```{r}
obj <- query(allNode_Map, as.character(1))
obj
```
